{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# main_path = './drive/MyDrive/NewsGeneration-NLP-Teknofest'\n",
    "main_path = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../Preprocess-Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from category_reducer import category_reducer\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow import expand_dims\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2, mobilenet_v2\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, concatenate\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Title                100 non-null    object\n",
      " 1   Content              100 non-null    object\n",
      " 2   Content_url          100 non-null    object\n",
      " 3   News_type            100 non-null    object\n",
      " 4   Day_month_year_hour  100 non-null    object\n",
      " 5   Img_url              100 non-null    object\n",
      " 6   img_path             100 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 5.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    # Load the data from the csv file and reduce the number of categories\n",
    "    '''\n",
    "    data_path = main_path + \"/Data/news-data-with-imgs.csv\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    df = data.copy()\n",
    "\n",
    "    # Reducing the number of categories\n",
    "    df = category_reducer(df)\n",
    "\n",
    "    print(df.info())\n",
    "\n",
    "    df = df[['Content', 'Title', 'img_path']]\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# df = df.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessImg:\n",
    "    '''\n",
    "    # Class to preprocess the images and get the size information of the images.\n",
    "    '''\n",
    "    def __init__(self, images_path):\n",
    "        self.images_path = images_path\n",
    "        self.loaded_imgs = []\n",
    "        self.img_arrays = []\n",
    "\n",
    "    def load_img(self, target_size):\n",
    "        for img in os.listdir(self.images_path):\n",
    "            img = load_img(os.path.join(self.images_path, img), target_size=target_size)\n",
    "            self.loaded_imgs.append(img)\n",
    "\n",
    "        return self.loaded_imgs\n",
    "\n",
    "    def img_to_array(self):\n",
    "        '''\n",
    "        # Load the images from the directory\n",
    "\n",
    "        Returns:\n",
    "            - loaded_imgs: List of loaded images\n",
    "        '''\n",
    "        for img in self.loaded_imgs:\n",
    "            img = img_to_array(img)\n",
    "            self.img_arrays.append(img)\n",
    "        return np.array(self.img_arrays) / 255.0\n",
    "\n",
    "    def get_size_info(self, loaded_imgs):\n",
    "        '''\n",
    "        # Get the size information of the images\n",
    "\n",
    "        Args:\n",
    "            - loaded_imgs: list of loaded images\n",
    "\n",
    "        Returns:\n",
    "            - weights_mean: Mean of the weights of the images\n",
    "            - weights_std: Standard deviation of the weights of the images\n",
    "            - heights_mean: Mean of the heights of the images\n",
    "            - heights_std: Standard deviation of the heights of the images\n",
    "        '''\n",
    "        img_weights = []\n",
    "        img_heights = []\n",
    "        for size in loaded_imgs:\n",
    "            img_weights.append(size.size[0])\n",
    "            img_heights.append(size.size[1])\n",
    "\n",
    "        weights_mean = np.mean(img_weights)\n",
    "        weights_std = np.std(img_weights)\n",
    "\n",
    "        heights_mean = np.mean(img_heights)\n",
    "        heights_std = np.std(img_heights)\n",
    "\n",
    "        return weights_mean, weights_std, heights_mean, heights_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean width: 224.0\n",
      "Standard deviation of widths: 0.0\n",
      "Mean height: 224.0\n",
      "Standard deviation of heights: 0.0\n",
      "==\n",
      "Image shapes:  (100, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "img_preprocessor = PreprocessImg(main_path + \"/Data/imgs/\")\n",
    "\n",
    "loaded_imgs = img_preprocessor.load_img(target_size=(224, 224))\n",
    "image_data = img_preprocessor.img_to_array()\n",
    "\n",
    "widths_mean, widths_std, heights_mean, heights_std = img_preprocessor.get_size_info(loaded_imgs)\n",
    "\n",
    "print(\"Mean width:\", widths_mean)\n",
    "print(\"Standard deviation of widths:\", widths_std)\n",
    "print(\"Mean height:\", heights_mean)\n",
    "print(\"Standard deviation of heights:\", heights_std)\n",
    "print(\"==\")\n",
    "print('Image shapes: ', image_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessText(Tokenizer):\n",
    "    '''\n",
    "    # Class to tokenize and pad the text data\n",
    "\n",
    "    Args:\n",
    "        - data: The text data list or series to be tokenized and padded\n",
    "    '''\n",
    "    def __init__(self, data):\n",
    "        super().__init__(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "        self.start_mark = '<start> '\n",
    "        self.end_mark = ' <end>'\n",
    "\n",
    "        data = data.apply(lambda x: self.start_mark + x + self.end_mark)\n",
    "\n",
    "        self.fit_on_texts(data)\n",
    "        self.tokens = self.texts_to_sequences(data)\n",
    "\n",
    "        self.numbers_of_words = [len(token) for token in self.tokens]\n",
    "        self.max_tokens = max(self.numbers_of_words)\n",
    "\n",
    "        self.padded_tokens = pad_sequences(self.tokens, padding='post', truncating='post')\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    # re-padding the tokens\n",
    "    def re_pad(self, max_tokens=None):\n",
    "        '''\n",
    "        # Re-pad the tokens to a new maximum length\n",
    "\n",
    "        Args:\n",
    "            - max_tokens: The new maximum lenght of the tokens\n",
    "        '''\n",
    "\n",
    "        self.padded_tokens = pad_sequences(self.tokens, maxlen=max_tokens, padding='post', truncating='post')\n",
    "        return self.padded_tokens\n",
    "    def get_info(self):\n",
    "        '''\n",
    "        # Get the information about the tokenized and padded data\n",
    "        '''\n",
    "        print(\"Max tokens: \", self.max_tokens)\n",
    "        print(\"Mean tokens: \", int(np.mean(self.numbers_of_words)))\n",
    "        print(\"Standard deviation of tokens: \", int(np.std(self.numbers_of_words)))\n",
    "        print(\"Vocabulary Size: \", len(self.word_index) +1 )\n",
    "        print('Shape of padded tokens: ', self.padded_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens:  26\n",
      "Mean tokens:  8\n",
      "Standard deviation of tokens:  3\n",
      "Vocabulary Size:  563\n",
      "Shape of padded tokens:  (100, 26)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreprocessText(df['Title'])\n",
    "\n",
    "padded_tokens = tokenizer.padded_tokens\n",
    "max_tokens = tokenizer.max_tokens\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "start_token = tokenizer.word_index[tokenizer.start_mark.strip()]\n",
    "end_token = tokenizer.word_index[tokenizer.end_mark.strip()]\n",
    "\n",
    "tokenizer.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images, X_texts, y_texts = [], [], []\n",
    "for img, seq in zip(image_data, padded_tokens):\n",
    "    for i in range(1, len(seq)):\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_tokens)[0]\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        X_images.append(img)\n",
    "        X_texts.append(in_seq)\n",
    "        y_texts.append(out_seq)\n",
    "\n",
    "X_images, X_texts, y_texts = np.array(X_images), np.array(X_texts), np.array(y_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "x = base_model(image_input)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "image_embedding = Dense(32, activation='relu')(x)\n",
    "\n",
    "# Decoder\n",
    "text_input = Input(shape=(None,))\n",
    "text_embedding = Embedding(vocab_size, 300)(text_input)\n",
    "text_lstm = LSTM(256)(text_embedding)\n",
    "\n",
    "combined = concatenate([image_embedding, text_lstm])\n",
    "output = Dense(vocab_size, activation='softmax')(combined)\n",
    "\n",
    "model = Model(inputs=[image_input, text_input], outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 90s 2s/step - loss: 3.1339\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 80s 2s/step - loss: 2.0125\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 83s 2s/step - loss: 1.8130\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 86s 2s/step - loss: 1.7100\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 86s 2s/step - loss: 1.5894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ca853f640>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "\n",
    "model.fit([X_images, X_texts], y_texts,\n",
    "          epochs=5, batch_size=64,\n",
    "          callbacks=[es]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(test_image):\n",
    "  initial_caption = '<start>'\n",
    "  max_caption_length = 20\n",
    "\n",
    "  initial_caption_seq = tokenizer.texts_to_sequences([initial_caption])[0]\n",
    "  initial_caption_seq = pad_sequences([initial_caption_seq], maxlen=max_tokens)\n",
    "\n",
    "  final_caption = []\n",
    "\n",
    "  while True:\n",
    "      predictions = model.predict([np.expand_dims(test_image, axis=0), initial_caption_seq])\n",
    "\n",
    "      predicted_word_index = np.argmax(predictions)\n",
    "\n",
    "      predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "\n",
    "      if predicted_word == '<end>':\n",
    "          break\n",
    "\n",
    "      final_caption.append(predicted_word)\n",
    "\n",
    "      initial_caption_seq = pad_sequences([initial_caption_seq[0].tolist() + [predicted_word_index]], maxlen=max_tokens)\n",
    "\n",
    "  final_caption = ' '.join(final_caption)\n",
    "\n",
    "  plt.imshow(test_image)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "  print()\n",
    "  print(final_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_images\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[68], line 15\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(test_image)\u001b[0m\n\u001b[0;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([np\u001b[38;5;241m.\u001b[39mexpand_dims(test_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), initial_caption_seq])\n\u001b[0;32m     13\u001b[0m predicted_word_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions)\n\u001b[1;32m---> 15\u001b[0m predicted_word \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_word\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_word_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_word \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "generate_text(X_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
