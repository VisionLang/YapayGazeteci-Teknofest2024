{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "sys.path.append('../Preprocess-Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "from category_reducer import category_reducer\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    # Load the data from the csv file and reduce the number of categories\n",
    "    '''\n",
    "    data_path = \"../Data/news-data-with-imgs.csv\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    df = data.copy()\n",
    "\n",
    "    # Reducing the number of categories \n",
    "    df = category_reducer(df)\n",
    "\n",
    "    print(df.info())\n",
    "\n",
    "    df = df[['Content', 'Title', 'img_path']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Title                100 non-null    object\n",
      " 1   Content              100 non-null    object\n",
      " 2   Content_url          100 non-null    object\n",
      " 3   News_type            100 non-null    object\n",
      " 4   Day_month_year_hour  100 non-null    object\n",
      " 5   Img_url              100 non-null    object\n",
      " 6   img_path             100 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 5.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessImg:\n",
    "    '''\n",
    "    # Class to preprocess the images and get the size information of the images.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.images_path = \"../Data/imgs/\"\n",
    "        \n",
    "    def load_img(self):\n",
    "        '''\n",
    "        # Load the images from the directory\n",
    "\n",
    "        Returns:\n",
    "            - loaded_imgs: List of loaded images\n",
    "        '''\n",
    "        loaded_imgs = []\n",
    "        for img in os.listdir(self.images_path):\n",
    "            loaded_imgs.append(image.load_img(os.path.join(self.images_path, img)))\n",
    "        return loaded_imgs\n",
    "\n",
    "    def get_size_info(self, loaded_imgs):\n",
    "        '''\n",
    "        # Get the size information of the images \n",
    "\n",
    "        Args:\n",
    "            - loaded_imgs: list of loaded images\n",
    "\n",
    "        Returns:\n",
    "            - weights_mean: Mean of the weights of the images\n",
    "            - weights_std: Standard deviation of the weights of the images\n",
    "            - heights_mean: Mean of the heights of the images\n",
    "            - heights_std: Standard deviation of the heights of the images\n",
    "        '''\n",
    "        img_weights = []\n",
    "        img_heights = []\n",
    "        for size in loaded_imgs:\n",
    "            img_weights.append(size.size[0])\n",
    "            img_heights.append(size.size[1])\n",
    "        \n",
    "        weights_mean = np.mean(img_weights)\n",
    "        weights_std = np.std(img_weights)\n",
    "        \n",
    "        heights_mean = np.mean(img_heights)\n",
    "        heights_std = np.std(img_heights)\n",
    "\n",
    "        return weights_mean, weights_std, heights_mean, heights_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean width: 1029.85\n",
      "Standard deviation of widths: 355.39463628479257\n",
      "Mean height: 709.22\n",
      "Standard deviation of heights: 297.7434660911974\n"
     ]
    }
   ],
   "source": [
    "img_preprocessor = PreprocessImg()\n",
    "\n",
    "loaded_imgs = img_preprocessor.load_img()\n",
    "\n",
    "widths_mean, widths_std, heights_mean, heights_std = img_preprocessor.get_size_info(loaded_imgs)\n",
    "print(\"Mean width:\", widths_mean)\n",
    "print(\"Standard deviation of widths:\", widths_std)\n",
    "print(\"Mean height:\", heights_mean)\n",
    "print(\"Standard deviation of heights:\", heights_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessText(Tokenizer):\n",
    "    def __init__(self, data):\n",
    "        Tokenizer.__init__(self)\n",
    "\n",
    "        self.fit_on_texts(data)\n",
    "        self.tokens = self.texts_to_sequences(data)\n",
    "\n",
    "        self.numbers_of_words = [len(token) for token in self.tokens]\n",
    "        self.max_tokens = int(np.mean(self.numbers_of_words) + 2 * np.std(self.numbers_of_words))\n",
    "\n",
    "        self.padded_tokens = pad_sequences(self.tokens, maxlen=self.max_tokens, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens: 13\n",
      "Shape of padded tokens: (100, 13)\n"
     ]
    }
   ],
   "source": [
    "title_preprocessor = PreprocessText(df['Title'])\n",
    "\n",
    "title_padded_tokens = title_preprocessor.padded_tokens\n",
    "title_max_tokens = title_preprocessor.max_tokens\n",
    "\n",
    "print(\"Max tokens:\", title_max_tokens)\n",
    "print('Shape of padded tokens:', title_padded_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens: 406\n",
      "Shape of padded tokens: (100, 406)\n"
     ]
    }
   ],
   "source": [
    "content_preprocessor = PreprocessText(df['Content'])\n",
    "\n",
    "content_padded_tokens = content_preprocessor.padded_tokens\n",
    "content_max_tokens = content_preprocessor.max_tokens\n",
    "\n",
    "print(\"Max tokens:\", content_max_tokens)\n",
    "print('Shape of padded tokens:', content_padded_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
