{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from urllib.error import URLError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total news :  162460\n",
      "total days : 2222\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "past_dates = []\n",
    "\n",
    "for path in os.listdir('../Data/'):\n",
    "    if path.split('.')[1] == \"csv\":\n",
    "        past_dates += [\n",
    "            '/20' + date.split('-')[0].split('.')[::-1][0] + \n",
    "                '/' +date.split('-')[0].split('.')[::-1][1] + \n",
    "                    '/' + date.split('-')[0].split('.')[::-1][2]\n",
    "                        for date in pd.read_csv(\"../Data/\" + path)['Day_month_year_hour']\n",
    "                ]\n",
    "\n",
    "print(\"total news : \", len(past_dates))\n",
    "past_dates = list(set(past_dates))\n",
    "print(\"total days :\", len(past_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_day = 3000\n",
    "start_day = 0\n",
    "end_day = start_day + max_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hesaplanan tarihlerin listesi\n",
    "dates = []\n",
    "\n",
    "def main(start_day, end_day, getToday):\n",
    "    # Şu anki tarih ve saat bilgisini al\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    if(getToday):\n",
    "        # Şu anki yıl, ay ve gün bilgilerini al\n",
    "        year = str(current_date.year)\n",
    "        month = str(current_date.month)\n",
    "        day = str(current_date.day)\n",
    "\n",
    "        # Ay ve gün bilgilerini iki haneli yapma (01, 02, ..., 09)\n",
    "        if len(month) == 1:\n",
    "            month = \"0\" + month\n",
    "        if len(day) == 1:\n",
    "            day = \"0\" + day\n",
    "\n",
    "        # Bugünün tarihini \"/YYYY/MM/DD\" formatında listeye ekleme\n",
    "        dates.append(\"/\" + year + \"/\" + month + \"/\" + day)\n",
    "    \n",
    "    # Belirtilen aralıkta tarihleri oluşturma\n",
    "    for i in range(start_day, end_day):\n",
    "        # Güncel tarihten önceki günleri hesapla\n",
    "        previous_date = current_date - timedelta(days= i)\n",
    "\n",
    "        # Önceki tarihin yıl, ay ve gün bilgilerini al\n",
    "        year = str(previous_date.year)\n",
    "        month = str(previous_date.month)\n",
    "        day = str(previous_date.day)\n",
    "\n",
    "        # Ay ve gün bilgilerini iki haneli yapma (01, 02, ..., 09)\n",
    "        if len(month) == 1:\n",
    "            month = \"0\" + month\n",
    "        if len(day) == 1:\n",
    "            day = \"0\" + day\n",
    "\n",
    "        # Oluşturulan tarihi \"/YYYY/MM/DD\" formatında listeye ekleme\n",
    "        date = \"/\" + year + \"/\" + month + \"/\" + day\n",
    "\n",
    "        if date not in past_dates:\n",
    "            dates.append(date)\n",
    "\n",
    "    # Oluşturulan tarih listesini geri döndürme\n",
    "    return dates\n",
    "\n",
    "# Fonksiyonu çağırarak belirli bir gün aralığındaki tarihleri hesapla\n",
    "dates = list(set(main(start_day=start_day, end_day=end_day, getToday=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeoutException_count = 0\n",
    "webDriverException_count = 0\n",
    "urlError_count = 0\n",
    "\n",
    "# Belirli bir URL'den haber sayfasını çeken fonksiyon.\n",
    "def get_news(url=\"\"):\n",
    "    global timeoutException_count\n",
    "    global webDriverException_count \n",
    "    global urlError_count \n",
    "    \n",
    "    # Web sürücüsü (driver) dosya yolunu belirleme\n",
    "    driver_path = \"../chromedriver-win32/chromedriver.exe\"\n",
    "    \n",
    "    try:\n",
    "        # Ana haber sitesi URL'si\n",
    "        main_url = \"https://www.sabah.com.tr\"\n",
    "\n",
    "        # Belirtilen URL'yi ana URL ile birleştirme\n",
    "        full_url = main_url + url\n",
    "\n",
    "        # Selenium servisini başlatma\n",
    "        service = ChromeService(executable_path=driver_path)\n",
    "\n",
    "        # WebDriver'ı başlatma\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "\n",
    "        # Sayfa boyutunu ayarlama\n",
    "        driver.set_window_size(10, 10)\n",
    "\n",
    "        # Belirtilen URL'yi açma\n",
    "        driver.get(full_url)\n",
    "\n",
    "        # Sayfa kaynağını alıp BeautifulSoup kullanarak parse etme\n",
    "        source = driver.page_source\n",
    "        soup = BeautifulSoup(source, features=\"html.parser\")\n",
    "\n",
    "    except TimeoutException:\n",
    "        timeoutException_count += 1\n",
    "        return None\n",
    "\n",
    "    except WebDriverException:\n",
    "        webDriverException_count += 1\n",
    "        return None\n",
    "\n",
    "    except URLError:\n",
    "        print(f\"Bağlantı hatası: URLError\")\n",
    "        urlError_count += 1\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # WebDriver'ı kapatma\n",
    "        driver.quit()\n",
    "\n",
    "    # Parse edilmiş HTML içeriğini geri döndürme\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haber içeriğini alan fonksiyon.\n",
    "def get_content(content_soup, url=\"\"):\n",
    "    # Haber içeriğini tutacak boş bir string\n",
    "    news_content = \"\"   \n",
    "    # Belirli bir URL'ye özgü div yapısını içeren ana div'i bulma\n",
    "    top_div = content_soup.find_all(\"div\", attrs={\"class\": \"col-sm-12 view20\" + url})\n",
    "    # Eğer belirli bir div yapısı bulunursa devam et\n",
    "    if top_div is not None:\n",
    "        # Her bir ana div içinde dolaş\n",
    "        for div in top_div:\n",
    "            # İç içe geçmiş div'ler arasında, içerik div'ini bulma\n",
    "            inner_div = div.find(\"div\", attrs={\"class\": \"newsDetailText\"})\n",
    "            # Eğer içerik div'i bulunursa devam et\n",
    "            if inner_div is not None:\n",
    "                # İçerik div'inin içindeki detay div'ini bulma\n",
    "                detail_div = inner_div.find(\"div\", attrs={\"class\": \"newsBox selectionShareable\"})\n",
    "                # Eğer detay div'i bulunursa devam et\n",
    "                if detail_div is not None:\n",
    "                    # Detay div'i içindeki paragrafları bulma\n",
    "                    paragraphs = detail_div.find_all('p')\n",
    "                    # Eğer paragraflar bulunursa devam et\n",
    "                    if paragraphs is not None:\n",
    "                        # Her bir paragrafı dolaşarak haber içeriğini birleştirme\n",
    "                        for paragraph in paragraphs:\n",
    "                            if paragraph is not None:\n",
    "                                news_content += paragraph.text\n",
    "    # Oluşturulan haber içeriğini geri döndürme\n",
    "    return news_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'roza' URL'sinden haber içeriğini alan fonksiyon.\n",
    "def get_news_from_roza(content_url):\n",
    "    # Haber içeriğini tutacak boş bir string\n",
    "    news_content = \"\"\n",
    "\n",
    "    try:\n",
    "        # Belirli bir 'roza' URL'sine ait haber sayfasını çekme\n",
    "        soup = get_news(url=content_url)\n",
    "\n",
    "        # Haber içeriğini içeren div yapısını bulma\n",
    "        paragraphs = soup.find(\"div\", class_=\"detail-text-area\")\n",
    "\n",
    "        # Eğer belirli bir div yapısı bulunursa devam et\n",
    "        if paragraphs is not None:\n",
    "            # Her bir paragrafı dolaş\n",
    "            for p in paragraphs.find_all(\"p\"):\n",
    "                # Eğer paragraf içinde \"strong\" etiketi yoksa, içeriği birleştir\n",
    "                if not p.find(\"strong\"):\n",
    "                    news_content += p.get_text(strip=True) + \" \"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "\n",
    "    # Oluşturulan haber içeriğini geri döndürme\n",
    "    return news_content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_from_finans(content_url):\n",
    "    news_content = \"\"\n",
    "    \n",
    "    soup = get_news(url=content_url)\n",
    "    \n",
    "    # Haber içeriğini içeren div yapısını bulma\n",
    "    paragraphs = soup.find(\"div\", attrs={\"class\" : \"detail-text-area\"})\n",
    "\n",
    "    # Eğer belirli bir div yapısı bulunursa devam et\n",
    "    if paragraphs is not None:\n",
    "        # Her bir paragrafı dolaş\n",
    "        for p in paragraphs.find_all(\"p\"):\n",
    "            # Eğer paragraf içinde \"strong\" etiketi yoksa, içeriği birleştir\n",
    "            if p.find(\"strong\") is None:\n",
    "                news_content += p.text\n",
    "\n",
    "    # Oluşturulan haber içeriğini geri döndürme\n",
    "    return news_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_urls = []  \n",
    "# Haber türlerini (örneğin, yaşam, ekonomi, spor) tutacak liste\n",
    "news_types = []  \n",
    "# Haber başlıklarını tutacak liste\n",
    "titles = []  \n",
    "# Haber tarih ve saat bilgilerini tutacak liste\n",
    "date_time_list = []  \n",
    "# Haber görsel URL'lerini tutacak liste\n",
    "image_urls = []  \n",
    "# Haber içeriklerini tutacak liste\n",
    "contents = []  \n",
    "\n",
    "datas = []\n",
    "def append_data(title=\"\", content='', content_url='', news_type='', date_time='', img_url=''):\n",
    "    datas.append([title, content, content_url, news_type, date_time, img_url])\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 days left\n"
     ]
    }
   ],
   "source": [
    "# Geçerli URL'nin indeksi\n",
    "current_url_index = 0  \n",
    "\n",
    "# Yasaklı URL'ler listesi, farklı HTML yapılarına sahip olanlar\n",
    "banned_urls = []\n",
    "# Yasaklı haberlerin listesi\n",
    "banned_news = []\n",
    "# Haber içeriklerinin URL'lerini tutacak liste\n",
    "\n",
    "# Yasaklı bağlantıların listesi\n",
    "banned_hrefs = []\n",
    "\n",
    "# Açilmayan sayfa sayısı\n",
    "counter = 0\n",
    "\n",
    "# Ana döngü, belirli bir tarih aralığındaki haberleri almak için kullanılır\n",
    "while True:\n",
    "    # Veriyi almak için URL'yi fonksiyona veriyoruz.\n",
    "    soup = get_news(url=\"/timeline\" + dates[current_url_index])\n",
    "    \n",
    "    # Sayfa açılmadıysa bu if çalışmaz ve current_url_index değişmez, böylece sayfa değişmez\n",
    "    if soup is not None and len(soup.find_all('div', attrs={'class': 'box'})) != 0:\n",
    "        \n",
    "        # İlerlemeyi adım adım takip etmek için\n",
    "        remaining_days = len(dates) - (current_url_index )\n",
    "        if remaining_days % 10 == 0:\n",
    "            print(f\"{remaining_days} days left\")  \n",
    "        \n",
    "        # news_boxes, o günün bütün haberlerini içeriyor\n",
    "        for box in soup.find_all('div', attrs={'class': 'box'}):\n",
    "            box_classes = box.get(\"class\")\n",
    "\n",
    "            # Her bir haberin class'i 'box', reklam olanların class'i 'box advert', uzunluğu 2 olduğu için reklamları almıyoruz\n",
    "            if len(box_classes) == 2:\n",
    "                pass\n",
    "\n",
    "            # Eğer class sadece tek bir değerden oluşuyorsa ('box') o zaman bu haberi alıyoruz\n",
    "            else:\n",
    "                # Haber içeriğinin URL'sini al\n",
    "                content_url = box.find('a').get(\"href\")\n",
    "                head_url = content_url.split(\"/\")[1]        \n",
    "\n",
    "                # Haberin türünü al\n",
    "                news_type = box.find_all(\"div\", attrs={\"class\": \"info\"})[0].find(\"a\").text.strip()\n",
    "                \n",
    "                # Haber başlığını al\n",
    "                title = box.find('strong').text.strip() \n",
    "                \n",
    "                # Haber tarih ve saat bilgisini al\n",
    "                current_date = box.find('div', {'class': \"info\"}).find(\"label\").text.strip()\n",
    "                date_time = current_date.split(\" \")[0].split(\",\")[0] + '-' + current_date.split(\" \")[-1]\n",
    "                \n",
    "                # Haber görsel URL'sini al\n",
    "                image_url = box.find(\"img\").get('src')\n",
    "\n",
    "                # URL 'roza' ile başlıyorsa, özel bir fonksiyon kullanarak haber içeriğini al\n",
    "                if head_url == \"roza\":\n",
    "\n",
    "                    roza_news = get_news_from_roza(content_url)\n",
    "                    data_arrays = append_data(title=title, content=roza_news, content_url=content_url, news_type=news_type, date_time=date_time, img_url=image_url)\n",
    "\n",
    "                # 'roza' ile başlamayan ve yasaklı URL'ler içinde olmayan haber başlıklarını al\n",
    "                elif head_url not in banned_urls:\n",
    "                    content_soup = get_news(url=content_url)\n",
    "                    if content_soup is not None:\n",
    "                        \n",
    "                        content_check = content_soup.find_all(\"div\", attrs={\"class\": \"col-sm-12 view20 pagedItems\"})\n",
    "\n",
    "                        # Eğer haber içeriğinde birden fazla fotoğraf varsa class yapısı değişiyor ve burada o haberin bu sayfa yapısına sahip olup \n",
    "                        # olmadığını kontrol ediyoruz \n",
    "                        if len(content_check) == 0:\n",
    "                            news_content = get_content(content_soup=content_soup)\n",
    "                            data_arrays = append_data(title=title, content=news_content, content_url=content_url, news_type=news_type, date_time=date_time, img_url=image_url)\n",
    "                        # Eğer haber birden fazla fotoğraf ve içerikten oluşuyorsa bu şart sağlanır\n",
    "                        else:    \n",
    "                            news_content = get_content(content_soup=content_soup, url=\" pagedItems\")\n",
    "                            data_arrays = append_data(title=title, content=news_content, content_url=content_url, news_type=news_type, date_time=date_time, img_url=image_url)\n",
    "                \n",
    "                elif head_url == \"finans\":\n",
    "                    \n",
    "                    finans_news = get_news_from_finans(content_url)\n",
    "                    data_arrays = append_data(title=title, content=finans_news, content_url=content_url, news_type=news_type, date_time=date_time, img_url=image_url)\n",
    "                    \n",
    "                else:\n",
    "                    # Eğer URL 'roza', 'finans' ve 'spor' sayfalarına giriyorsa geç\n",
    "                    banned_hrefs.append(content_url)\n",
    "                    \n",
    "        current_url_index += 1        \n",
    "\n",
    "\n",
    "    # Eğer sayfa açılmazsa current_url_index değişmez ve döngü tekrar aynı sayfayı deneyecektir\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    # İstenen gün sayısına ulaşıldığında döngüyü sonlandır\n",
    "    if current_url_index == len(dates):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeoutException_count : 0\n",
      "webDriverException_count : 207\n",
      "urlError_count : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"timeoutException_count :\" , timeoutException_count)\n",
    "print(\"webDriverException_count :\", webDriverException_count)\n",
    "print('urlError_count :', urlError_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Title', 'Content', 'Content_url', 'News_type', 'Day_month_year_hour', 'Img_url'])\n",
    "\n",
    "for data in data_arrays: \n",
    "    df.loc[len(df)] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Data/data_33.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11302 entries, 0 to 11301\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Title                11302 non-null  object\n",
      " 1   Content              7696 non-null   object\n",
      " 2   Content_url          11302 non-null  object\n",
      " 3   News_type            11302 non-null  object\n",
      " 4   Day_month_year_hour  11302 non-null  object\n",
      " 5   Img_url              11302 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 529.9+ KB\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv('../Data/data_33.csv').info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
